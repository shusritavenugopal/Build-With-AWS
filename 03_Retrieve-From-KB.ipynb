{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USE CASE - Build With AWS \n",
    "\n",
    "The Build with AWS app is a comprehensive guide to navigating Amazon Web Services (AWS), the leading global cloud provider with over 200 fully featured services. It addresses the challenge of selecting the right services by allowing users to input project details such as descriptions, libraries, and algorithms. Using this input, the app analyzes project requirements and generates a curated list of AWS services that align best with these needs.\n",
    "\n",
    "The app provides step-by-step guidance on integrating these recommended services into projects, covering setup, configuration, and usage for effective implementation. Additionally, it offers access to AWS blogs and resources to aid project development. A built-in chatbot offers real-time support, allowing users to ask questions and receive assistance on AWS services and configurations.\n",
    "\n",
    "We can effectively implement the solution using the Retrieval Augmented Generation (RAG) pattern. RAG involves retrieving data from sources external to the language model (non-parametric) and enhancing prompts by incorporating relevant retrieved data into the context. In this case, we are efficiently performing RAG on the knowledge base created using the console or SDK.\n",
    "\n",
    "In this notebook, we'll query the knowledge base to retrieve the required number of document chunks through similarity search. These chunks will augment the prompt with relevant documents, forming the input query for Anthropic Claude V2 to generate responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisite\n",
    "\n",
    "Before answering questions, ensure that the following steps are completed for document processing and ingestion into the vector database:\n",
    "\n",
    "1. **Load Documents into Knowledge Base**\n",
    "   - Connect your S3 bucket as the data source to the knowledge base.\n",
    "\n",
    "2. **Ingestion Process**\n",
    "   - Knowledge bases will segment documents into smaller chunks based on the chosen strategy.\n",
    "   - Generate embeddings from these chunks and store them in the associated vector store.\n",
    "\n",
    "3. **Execution with Notebook 0_create_ingest_documents_test_kb.ipynb**\n",
    "   - Utilize notebook `02_Create-Knowledge-Base.ipynb` to automate the ingestion process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Walkthrough\n",
    "We will leverage the Retrieve API provided by Knowledge Bases for Amazon Bedrock to convert user queries into embeddings, perform semantic search on the knowledge base, and retrieve relevant results. This API offers enhanced flexibility to customize workflows atop semantic search outcomes. The output from the Retrieve API includes retrieved text chunks, source data location type and URI, and relevance scores for each retrieval.\n",
    "\n",
    "Subsequently, we will augment these text chunks with the original prompt. This augmented input will then be processed through the anthropic.claude-3-sonnet-20240229-v1:0 model using prompt engineering techniques tailored to your specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "To run this notebook you would need to install following packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install boto3 --force-reinstall --quiet\n",
    "%pip install botocore --force-reinstall --quiet\n",
    "%pip install langchain --force-reinstall --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restart the kernel with the updated packages that are installed through the dependencies above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r kb_id\n",
    "# kb_id = \"<knowledge base id>\" If you have already created knowledge base, comment the `store -r kb_id` and provide knowledge base id here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To initialize the bedrock client and set up the anthropic.claude-3-sonnet-20240229-v1:0 language model for query completions using the RAG pattern alongside the knowledge base, follow these steps:\n",
    "\n",
    "1. **Import Necessary Libraries**:\n",
    "   - Import required libraries including langchain for bedrock model selection and llama index for managing service contexts.\n",
    "\n",
    "2. **Initialize Service Context**:\n",
    "   - Create a service context using llama index to store instances of language models (LLM) and embedding models. This context will be used later to evaluate responses from the Q&A application.\n",
    "\n",
    "3. **Set anthropic.claude-3-sonnet-20240229-v1:0 as Language Model**:\n",
    "   - Initialize anthropic.claude-3-sonnet-20240229-v1:0 as the chosen large language model (LLM). This model will facilitate query completions using the Retrieval Augmented Generation (RAG) pattern in conjunction with the retrieved text chunks obtained through the retrieve API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pprint\n",
    "from botocore.client import Config\n",
    "import json\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 0})\n",
    "bedrock_client = boto3.client('bedrock-runtime', region_name = region)\n",
    "bedrock_agent_client = boto3.client(\"bedrock-agent-runtime\",\n",
    "                              config=bedrock_config, region_name = region)\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve API with foundation models from Amazon Bedrock\n",
    "\n",
    "The \"retrieve\" function helps you find information by converting your questions into codes, searching for similar ideas. After searching, it brings back pieces of text that are similar to what you asked. Along with these text pieces, it also tells you where these ideas came from (like a web link) and how close they are to what you asked for (relevancy score). \n",
    "\n",
    "You can also choose how it searches using a setting called \"overrideSearchType.\" If you let it decide on its own (the default setting), it picks the best way to find what you want. But if you set \"overrideSearchType\" to \"HYBRID\" or \"SEMANTIC,\" you can tell it to use a specific way to search instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, kbId, numberOfResults=5):\n",
    "    return bedrock_agent_client.retrieve(\n",
    "        retrievalQuery= {\n",
    "            'text': query\n",
    "        },\n",
    "        knowledgeBaseId=kbId,\n",
    "        retrievalConfiguration= {\n",
    "            'vectorSearchConfiguration': {\n",
    "                'numberOfResults': numberOfResults,\n",
    "                'overrideSearchType': \"HYBRID\", # optional\n",
    "            }\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before querying responses from the initialized large language model (LLM), start by setting up or initializing your knowledge base ID.\n",
    "\n",
    "After this setup, proceed to call the retrieve API. Provide the knowledge base ID, specify the number of results you want to receive, and include your query as parameters.\n",
    "\n",
    "When you receive the results, each text chunk will come with a score. This score indicates how closely each chunk matches your query, helping you understand its relevance and correlation to the original question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the products and services that could be used for Semi Structured Database application?\"\n",
    "response = retrieve(query, kb_id, 5)\n",
    "retrievalResults = response['retrievalResults']\n",
    "pp.pprint(retrievalResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the text chunks from the retrieveAPI response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch context from the response\n",
    "def get_contexts(retrievalResults):\n",
    "    contexts = []\n",
    "    for retrievedResult in retrievalResults: \n",
    "        contexts.append(retrievedResult['content']['text'])\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = get_contexts(retrievalResults)\n",
    "pp.pprint(contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt specific to the model to personalize responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Human: Analyze the requirement in query to briefly introduce and suggest relevant AWS Products, AWS services and Amazon services. Write a step-by-step usage of Relevant AWS Services based on the project. Suggest blogs and relevant resources from AWS to support the user to work with Relevant AWS Services and How I can use these AWS Services in my project?\n",
    "<context>\n",
    "{contexts}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{query}\n",
    "</question>\n",
    "\n",
    "The response should be specific and use statistics or numbers when possible.\n",
    "\n",
    "Assistant:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke foundation model from Amazon Bedrock\n",
    "\n",
    "Using anthropic.claude-3-sonnet-20240229-v1:0 foundation model from Amazon Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# payload with model paramters\n",
    "messages=[{ \"role\":'user', \"content\":[{'type':'text','text': prompt.format(contexts, query)}]}]\n",
    "sonnet_payload = json.dumps({\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens\": 512,\n",
    "    \"messages\": messages,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 1\n",
    "        }  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelId = 'anthropic.claude-3-sonnet-20240229-v1:0' # change this to use a different version from the model provider\n",
    "accept = 'application/json'\n",
    "contentType = 'application/json'\n",
    "response = bedrock_client.invoke_model(body=sonnet_payload, modelId=modelId, accept=accept, contentType=contentType)\n",
    "response_body = json.loads(response.get('body').read())\n",
    "response_text = response_body.get('content')[0]['text']\n",
    "\n",
    "pp.pprint(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangChain Integration\n",
    "\n",
    "1. Query the knowledge base to get the desired number of document chunks based on similarity search, \n",
    "2. Integrate it with LangChain retriever and use Anthropic Claude 3 Sonnet model for answering questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llms.bedrock import Bedrock\n",
    "from langchain_community.chat_models.bedrock import BedrockChat\n",
    "from langchain.retrievers.bedrock import AmazonKnowledgeBasesRetriever\n",
    "\n",
    "llm = BedrockChat(model_id=modelId, \n",
    "                  client=bedrock_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Analyze the requirement in query to briefly introduce and suggest relevant AWS Products, AWS services and Amazon services. Write a step-by-step usage of Relevant AWS Services based on the project. Suggest blogs and relevant resources from AWS to support the user to work with Relevant AWS Services and How I can use these AWS Services in my project?\"\n",
    "retriever = AmazonKnowledgeBasesRetriever(\n",
    "        knowledge_base_id=kb_id,\n",
    "        retrieval_config={\"vectorSearchConfiguration\": \n",
    "                          {\"numberOfResults\": 4,\n",
    "                           'overrideSearchType': \"SEMANTIC\", # optional\n",
    "                           }\n",
    "                          },\n",
    "        # endpoint_url=endpoint_url,\n",
    "        # region_name=region,\n",
    "        # credentials_profile_name=\"<profile_name>\",\n",
    "    )\n",
    "docs = retriever.get_relevant_documents(\n",
    "        query=query\n",
    "    )\n",
    "pp.pprint(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt specific to the model to personalize responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Human: Analyze the requirement in query to briefly introduce and suggest relevant AWS Products, AWS services and Amazon services. Write a step-by-step usage of Relevant AWS Services based on the project. Suggest blogs and relevant resources from AWS to support the user to work with Relevant AWS Services and How I can use these AWS Services in my project?\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "The response should be specific and use statistics or numbers when possible.\n",
    "\n",
    "Assistant:\"\"\"\n",
    "claude_prompt = PromptTemplate(template=PROMPT_TEMPLATE, \n",
    "                               input_variables=[\"context\",\"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integrating the retriever and the LLM defined above with RetrievalQA Chain to build the Q&A application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": claude_prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = qa.invoke(query)\n",
    "pp.pprint(answer)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
